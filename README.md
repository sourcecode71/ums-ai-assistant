# ğŸ“ UMS AI Assistant

The **UMS AI Assistant** is an AI-powered document retrieval and question-answering system built using **FastAPI**, **RAG (Retrieval-Augmented Generation)**, and a **Vector Database**.  
It helps university staff query and summarize **financial and administrative documents** securely and efficiently.

---

## ğŸš€ Project Overview

Universities often have thousands of complex financial documents (e.g., audit reports, student billing summaries, purchase orders).  
This project allows you to upload these PDF documents, convert them into searchable vector embeddings, and use an AI model to answer natural language questions based on document content.

### ğŸ§  How it works
1. **Upload PDFs** â€” Admin users can upload one or multiple PDF documents.  
2. **Ingestion Pipeline** â€” Extracts text, splits into chunks, embeds them, and stores them in a VectorDB.  
3. **Query** â€” Users ask questions in natural language.  
4. **RAG Pipeline** â€” The system retrieves the most relevant text chunks and sends them to an LLM (e.g., GPT-4 or local LLM).  
5. **Response** â€” The model generates an accurate answer with source references.

---

## ğŸ—ï¸ System Architecture

               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        User (Frontend)      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚   REST API
                              â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚      FastAPI (RAG)     â”‚
                  â”‚   /ingest   /query     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â–¼                           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Embedding Model  â”‚       â”‚   Vector Database   â”‚
      â”‚ (OpenAI or local) â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ (Weaviate/Chroma)  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚     LLM API  â”‚
                       â”‚ (GPT-4 / etc)â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## âš™ï¸ Features

- ğŸ“„ PDF ingestion and text extraction  
- ğŸ§© Automatic text chunking with overlap  
- ğŸ§  Embedding-based semantic search  
- ğŸ—„ï¸ Vector database integration (Weaviate / Chroma / Pinecone)  
- ğŸ’¬ FastAPI endpoints for ingestion & querying  
- ğŸ” Supports secure, private deployment  
- ğŸª¶ Lightweight, modular architecture for easy customization  

---

## ğŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| Backend Framework | [FastAPI](https://fastapi.tiangolo.com/) |
| AI Framework | [LangChain](https://www.langchain.com/) |
| Vector Database | Weaviate / Chroma / Pinecone |
| Embeddings | OpenAI / Sentence-Transformers |
| PDF Parser | pdfplumber / PyMuPDF |
| LLM | GPT-4 / LLaMA / Local models |
| Deployment | Docker / Uvicorn / Nginx |

---

## ğŸ§© Folder Structure


---

## ğŸ§  Environment Setup

### Option 1: Using Docker (Recommended)
If you prefer to use Docker, clone the repository and skip to the Docker running instructions below.

### Option 2: Local Setup

#### 1. Clone the repository
```bash
git clone https://github.com/yourusername/ums-ai-assistant.git
cd ums-ai-assistant
```

#### 2. Install dependencies
```bash
pip install -r requirements.txt
```

#### 3. Set up environment variables
Copy the `.env` file and configure your API keys and settings.

---

## ğŸš€ Running the Application

### Using Docker (Recommended)
To run the application using Docker Compose:
```bash
docker-compose up --build
```

Or build and run with Docker directly:
```bash
docker build -t ums-ai-assistant .
docker run -p 8000:8000 ums-ai-assistant
```

### Local Development
To run the application in development mode with auto-reload:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Local Production
For production deployment with multiple workers:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

### ğŸ” Access Swagger UI
Once the application is running, you can access the API documentation:

- **Interactive Swagger UI (Recommended)**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **Alternative Swagger UI**: [http://localhost:8000/redoc](http://localhost:8000/redoc)
- **OpenAPI JSON Specification**: [http://localhost:8000/openapi.json](http://localhost:8000/openapi.json)
